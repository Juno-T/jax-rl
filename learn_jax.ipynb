{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.3721109   0.26423115 -0.18252768 -0.7368197  -0.44030377 -0.1521442\n",
      " -0.67135346 -0.5908641   0.73168886  0.5673026 ]\n"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "x = random.normal(key, (10,))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.83 ms ± 289 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "size = 3000\n",
    "x = random.normal(key, (size, size), dtype=jnp.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()  # runs on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ms ± 177 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.42 ms ± 24 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import device_put\n",
    "\n",
    "x = np.random.normal(size=(size, size)).astype(np.float32)\n",
    "x = device_put(x) # put nd array to gpu\n",
    "%timeit jnp.dot(x, x.T).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.16 ms ± 3.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.6 µs ± 512 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "selu_jit = jit(selu) # (@jit decoration) compile multiple operation together with XLA\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661194 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_differences(f, x):\n",
    "  eps = 1e-3\n",
    "  return jnp.array([(f(x + eps * v) - f(x - eps * v)) / (2 * eps)\n",
    "                   for v in jnp.eye(len(x))])\n",
    "\n",
    "\n",
    "print(first_finite_differences(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0353256\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev\n",
    "def hessian(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = random.normal(key, (150, 100))\n",
    "batched_x = random.normal(key, (10, 100))\n",
    "\n",
    "def apply_matrix(v):\n",
    "  return jnp.dot(mat, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "2.06 ms ± 248 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually batched\n",
      "The slowest run took 4.13 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "261 µs ± 154 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Manually batched\n",
      "57.8 µs ± 9.76 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def batched_apply_matrix(v_batched):\n",
    "  return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()\n",
    "\n",
    "@jit\n",
    "def batched_apply_matrix(v_batched):\n",
    "  return jnp.dot(v_batched, mat.T)\n",
    "\n",
    "print('Manually batched')\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-vectorized with vmap\n",
      "107 µs ± 19.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(v_batched):\n",
    "  return vmap(apply_matrix)(v_batched)\n",
    "\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Think in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/juno/work/playground/jax-rl/learn_jax.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000022vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# JAX: immutable arrays\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000022vscode-remote?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marange(\u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000022vscode-remote?line=2'>3</a>\u001b[0m x[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "File \u001b[0;32m~/work/playground/venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:6795\u001b[0m, in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m   <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py?line=6789'>6790</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unimplemented_setitem\u001b[39m(\u001b[39mself\u001b[39m, i, x):\n\u001b[1;32m   <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py?line=6790'>6791</a>\u001b[0m   msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object does not support item assignment. JAX arrays are \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py?line=6791'>6792</a>\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mimmutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py?line=6792'>6793</a>\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mor another .at[] method: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py?line=6793'>6794</a>\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py?line=6794'>6795</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)))\n",
      "\u001b[0;31mTypeError\u001b[0m: '<class 'jaxlib.xla_extension.DeviceArray'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html"
     ]
    }
   ],
   "source": [
    "# JAX: immutable arrays\n",
    "x = jnp.arange(10)\n",
    "x[0] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10  1  2  3  4  5  6  7  8  9]\n"
     ]
    }
   ],
   "source": [
    "y = x.at[0].set(10)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX > LAX > XLA  \n",
    "\">\" = build on, higher level, less strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 3. 4. 4. 4. 4. 4. 4. 4. 4. 3. 1.]\n",
      "[1. 3. 4. 4. 4. 4. 4. 4. 4. 4. 3. 1.]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([1, 2, 1])\n",
    "y = jnp.ones(10)\n",
    "print(jnp.convolve(x, y))\n",
    "\n",
    "from jax import lax\n",
    "result = lax.conv_general_dilated(\n",
    "    x.reshape(1, 1, 3).astype(float),  # note: explicit promotion\n",
    "    y.reshape(1, 1, 10),\n",
    "    window_strides=(1,),\n",
    "    padding=[(len(y) - 1, len(y) - 1)])  # equivalent of padding='full' in NumPy\n",
    "print(result[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jit, static&traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def norm(X):\n",
    "  X = X - X.mean(0)\n",
    "  return X / X.std(0)\n",
    "norm_compiled = jit(norm) # require static shape\n",
    "\n",
    "np.random.seed(1701)\n",
    "X = jnp.array(np.random.rand(10000, 10))\n",
    "np.allclose(norm(X), norm_compiled(X), atol=1E-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369 µs ± 9.64 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "85.9 µs ± 6.14 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit norm(X).block_until_ready()\n",
    "%timeit norm_compiled(X).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-0.16529104, -0.5616348 , -0.02158209, -0.37602973,\n",
       "             -0.82070136], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_negatives(x):\n",
    "  return x[x < 0] # not static\n",
    "\n",
    "x = jnp.array(np.random.randn(10))\n",
    "get_negatives(x) # op-by-op mode\n",
    "# jit(get_negatives)(x) # jit mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running f():\n",
      "  x = Traced<ShapedArray(float32[3,4])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  y = Traced<ShapedArray(float32[4])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  result = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=0/1)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 5.291437 ,  2.4938517, 10.846224 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def f(x, y):\n",
    "  print(\"Running f():\")\n",
    "  print(f\"  x = {x}\") # tracer object\n",
    "  print(f\"  y = {y}\")\n",
    "  result = jnp.dot(x + 1, y + 1)\n",
    "  print(f\"  result = {result}\")\n",
    "  return result\n",
    "\n",
    "x = np.random.randn(3, 4)\n",
    "y = np.random.randn(4)\n",
    "f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.805405  6.7191615 3.2264812]\n",
      "Running f():\n",
      "  x = Traced<ShapedArray(float32[3,4])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  y = Traced<ShapedArray(float32[4,1])>with<DynamicJaxprTrace(level=0/1)>\n",
      "  result = Traced<ShapedArray(float32[3,1])>with<DynamicJaxprTrace(level=0/1)>\n",
      "[[2.3823314]\n",
      " [1.3786898]\n",
      " [1.6461018]]\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.randn(3, 4)\n",
    "y2 = np.random.randn(4)\n",
    "print(f(x2, y2)) # nothing is printed as it has been compiled. Same input size = no-recompilation\n",
    "print(f(x2, np.random.randn(4,1))) # shape change = recompile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[3,4] b:f32[4]. let\n",
       "    c:f32[3,4] = add a 1.0\n",
       "    d:f32[4] = add b 1.0\n",
       "    e:f32[3] = dot_general[\n",
       "      dimension_numbers=(((1,), (0,)), ((), ()))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "    ] c d\n",
       "  in (e,) }"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import make_jaxpr\n",
    "\n",
    "def f(x, y):\n",
    "  return jnp.dot(x + 1, y + 1)\n",
    "\n",
    "make_jaxpr(f)(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function f at /tmp/ipykernel_1479/2422663986.py:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'neg'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/home/juno/work/playground/jax-rl/learn_jax.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000030vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m@jit\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000030vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x, neg):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000030vscode-remote?line=2'>3</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mx \u001b[39mif\u001b[39;00m neg \u001b[39melse\u001b[39;00m x\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000030vscode-remote?line=4'>5</a>\u001b[0m f(\u001b[39m1\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "\u001b[1;32m/home/juno/work/playground/jax-rl/learn_jax.ipynb Cell 31'\u001b[0m in \u001b[0;36mf\u001b[0;34m(x, neg)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000030vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m@jit\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000030vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(x, neg):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/juno/work/playground/jax-rl/learn_jax.ipynb#ch0000030vscode-remote?line=2'>3</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mx \u001b[39mif\u001b[39;00m neg \u001b[39melse\u001b[39;00m x\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/work/playground/venv/lib/python3.8/site-packages/jax/core.py:1046\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/core.py?line=1044'>1045</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[0;32m-> <a href='file:///~/work/playground/venv/lib/python3.8/site-packages/jax/core.py?line=1045'>1046</a>\u001b[0m   \u001b[39mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\nThe problem arose with the `bool` function. \nWhile tracing the function f at /tmp/ipykernel_1479/2422663986.py:1 for jit, this concrete value was not available in Python because it depends on the value of the argument 'neg'.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x, neg):\n",
    "  return -x if neg else x # Op flow contain branching that depends on input.\n",
    "\n",
    "f(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(-1, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jit, static_argnums=(1,)) # casting arg to static\n",
    "def f(x, neg):\n",
    "  print(neg)\n",
    "  return -x if neg else x\n",
    "\n",
    "f(1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(2, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(2, True)\n",
    "f(2, False) # re-compile since static arg is changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts:**\n",
    "* Just as values can be either static or traced, operations can be static or traced.\n",
    "* Static operations are evaluated at compile-time in Python; traced operations are compiled & evaluated at run-time in XLA.\n",
    "* *Use `numpy` for operations that you want to be static; use `jax.numpy` for operations that you want to be traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Traced<ShapedArray(float32[2,3])>with<DynamicJaxprTrace(level=0/1)>\n",
      "x.shape = (2, 3)\n",
      "jnp.array(x.shape).prod() = Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=0/1)>\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x):\n",
    "  print(f\"x = {x}\")\n",
    "  print(f\"x.shape = {x.shape}\")\n",
    "  print(f\"jnp.array(x.shape).prod() = {jnp.array(x.shape).prod()}\")\n",
    "  # comment this out to avoid the error:\n",
    "  # return x.reshape(jnp.array(x.shape).prod()) # reshape require static.\n",
    "\n",
    "x = jnp.ones((2, 3))\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "@jit\n",
    "def f(x):\n",
    "  return x.reshape((np.prod(x.shape),)) # reshape require static so use numpy!\n",
    "\n",
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.2 µs ± 15.3 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "52.2 µs ± 5.6 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    # print(x)\n",
    "    # print(y)\n",
    "    return jnp.dot(x,y)\n",
    "jit_f = jit(f)\n",
    "size = 101\n",
    "x = random.normal(key, (size,size))\n",
    "y = random.normal(key, (size,size))\n",
    "# print(f(x,y))\n",
    "# print(jit_f(x,y))\n",
    "%timeit f(x,y)\n",
    "%timeit jit_f(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c9c7a98df09ad5d09aed9c39f66cd404562c24fcd761bc8bc141afc0ba32c4d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
